{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "individual-original",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False # in case your autocomplete does not work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-member",
   "metadata": {},
   "source": [
    "What we did in the previous article does not follow a best practice. \n",
    "For example, I use the test set to do validation. \n",
    "I should randomly split the training set into a training set and a validation set.\n",
    "This is just one example of me being lazy and not following the best engineering practice to conduct a ML study.\n",
    "The other example is that I should store the optimal weights. \n",
    "There is a pattern in dealing with this problem, i.e., using a framework.\n",
    "For PyTorch, the go-to framework is PyTorch Lightning (PL). \n",
    "The goal of PL is to separate the engineering practice and the research practice.\n",
    "It automates the engineering practice, i.e., split a dataset, prepare a test set, calculate accuracy (we only calculate loss before), logging, move computation to GPUs or TPUs etc. \n",
    "We will use PL to redo our previous activity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-command",
   "metadata": {},
   "source": [
    "# PyTorch Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-tuning",
   "metadata": {},
   "source": [
    "To put it simply, PL is a wrapper of nn.Module of PyTorch. \n",
    "That is, PL adds more methods.\n",
    "In addition to init and forward, we should add (a common practice in PL):\n",
    "\n",
    "- training_step: self explanatory\n",
    "- validation_step: self explanatory\n",
    "- test_step: self explanatory\n",
    "- configure_optimizer: self explanatory\n",
    "- prepare_data: download dataset once (if needed)\n",
    "- setup: load dataset, transform, split data, copy data to GPUs, etc\n",
    "- train_dataloader: self explanatory\n",
    "- val_dataloader: self explanatory\n",
    "- test_dataloader: self explanatory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "false-quest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl # --> NEW\n",
    "from torchvision import datasets, transforms\n",
    "from torchmetrics.functional import accuracy\n",
    "# from pytorch_lightning.metrics.functional import accuracy # --> This is NEW\n",
    "from torch.utils.data import DataLoader, random_split # random_split is NEW\n",
    "\n",
    "import sps # from https://github.com/IssamLaradji/sps --> a new optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-idaho",
   "metadata": {},
   "source": [
    "if you learn a framework, IMHO it's better to see examples from the framework creator.\n",
    "For this tutorial, I follow: \n",
    "\n",
    "https://colab.research.google.com/github/PytorchLightning/pytorch-lightning/blob/master/notebooks/01-mnist-hello-world.ipynb#scrollTo=Mb0U5Rk2kLBy \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "passive-ecology",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, input_size, num_neurons_hidden1,  num_neurons_hidden2, output_size, optimizer='adam'):\n",
    "        # Copy paste from the previous article\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(input_size,num_neurons_hidden1)\n",
    "        self.hidden_layer1 = nn.Linear(num_neurons_hidden1,num_neurons_hidden2)\n",
    "        self.output_layer = nn.Linear(num_neurons_hidden2,output_size)\n",
    "        \n",
    "        tf_resize = transforms.Resize((28,28)) # make sure that all loaded images have these dimensions\n",
    "        tf_totensor = transforms.ToTensor() # Why? pytorch uses this datatype\n",
    "        tf_normalize = transforms.Normalize(mean=(.5,),std=(.5,)) # this is rather tricky to explain. I'll explain it after we compose all these transformations\n",
    "\n",
    "        self.tf_compose = transforms.Compose([tf_resize,tf_totensor,tf_normalize])\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # Copy paste from the previous article\n",
    "        y = nn.functional.relu(self.input_layer(x))\n",
    "        y = nn.functional.relu(self.hidden_layer1(y))\n",
    "        y = self.output_layer(y)\n",
    "        return y\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Copy paste from the previous article\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.view(inputs.shape[0],-1)\n",
    "        \n",
    "        outputs = self.forward(inputs)\n",
    "        loss = nn.functional.cross_entropy(outputs,labels) # --> NEW. Using nn.CrossEntropyLoss\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # This is new, but the structure is the same as training_step\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.view(inputs.shape[0],-1)\n",
    "        \n",
    "        outputs = self.forward(inputs)\n",
    "        loss = nn.functional.cross_entropy(outputs,labels)\n",
    "        \n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        acc = accuracy(preds, labels) # --> NEW\n",
    "        \n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # This is new, but the structure is the same as test_step\n",
    "        # but I replace val_loss --> test_loss etc\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.view(inputs.shape[0],-1)\n",
    "        \n",
    "        outputs = self.forward(inputs)\n",
    "        loss = nn.functional.cross_entropy(outputs,labels)\n",
    "        \n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        acc = accuracy(preds, labels)\n",
    "        \n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "        self.log('test_acc', acc, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        if self.optimizer == 'sps':\n",
    "            optimizer = sps.Sps(self.parameters()) # No learning rate to be set :D\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "    ####################\n",
    "    # DATA RELATED HOOKS\n",
    "    ####################\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download once\n",
    "        datasets.MNIST(root='./', train=True, download=True)\n",
    "        datasets.MNIST(root='./', train=False, download=True)\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        # split, transform, secretly move to GPU (if needed) by PL (not by us)\n",
    "        if stage == 'fit' or stage is None:\n",
    "            mnist_full = datasets.MNIST(root='./', train=True, transform=self.tf_compose)\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
    "            \n",
    "        if stage == 'test' or stage is None:\n",
    "            self.mnist_test = datasets.MNIST(root='./', train=False, transform=self.tf_compose)\n",
    "            \n",
    "    def train_dataloader(self): \n",
    "        return DataLoader(self.mnist_train, batch_size=100, num_workers=2)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=100, num_workers=2)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=100, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-dealing",
   "metadata": {},
   "source": [
    "Did you notice that we move all the nitty-gritty to pl.LightningModule? The training and validation steps can be done by the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "induced-admission",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1412\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type   | Params\n",
      "-----------------------------------------\n",
      "0 | input_layer   | Linear | 78.5 K\n",
      "1 | hidden_layer1 | Linear | 5.0 K \n",
      "2 | output_layer  | Linear | 510   \n",
      "-----------------------------------------\n",
      "84.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "84.1 K    Total params\n",
      "0.336     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ardimas/miniconda3/envs/pytorch_tutorial/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/ardimas/miniconda3/envs/pytorch_tutorial/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                not been set for this class (_ResultMetric). The property determines if `update` by\n",
      "                default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                achieved and we recommend setting this to `False`.\n",
      "                We provide an checking function\n",
      "                `from torchmetrics.utilities import check_forward_no_full_state`\n",
      "                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                default for now) or if `full_state_update=False` can be used safely.\n",
      "                \n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/ardimas/miniconda3/envs/pytorch_tutorial/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adb27fd03d9643ca8ab7903cc4af0197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pl.seed_everything(1412) # --> for consistency, change the number with your favorite number :D\n",
    "\n",
    "model = FNN(784, 100, 50, 10)\n",
    "\n",
    "# most basic trainer, uses good defaults (auto-tensorboard, checkpoints, logs, and more)\n",
    "try:\n",
    "    trainer = pl.Trainer(gpus=1,max_epochs=10)\n",
    "except Exception as e:\n",
    "    # most likely due to GPU, so fallback to non GPU\n",
    "    print(e)\n",
    "    trainer = pl.Trainer(max_epochs=10)\n",
    "    \n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-refund",
   "metadata": {},
   "source": [
    "We get all the progress bar as well as val_loss and val_acc values. The deault logger is TensorBoard (Note that many types of loggers out there). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dietary-decision",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 111424), started 0:09:49 ago. (Use '!kill 111424' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-78cdf8efd330e5b3\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-78cdf8efd330e5b3\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start tensorboard.\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-convergence",
   "metadata": {},
   "source": [
    "# Test our model with other handwritten digit dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-rally",
   "metadata": {},
   "source": [
    "Let's copy the custom dataset and loader from the third tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "changing-remainder",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from skimage import io, transform\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self,transform=None):\n",
    "        self.list_image_name = os.listdir('CUSTOM_DATASET')\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.list_image_name)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        \n",
    "        path = os.path.join('./CUSTOM_DATASET/',dataset.list_image_name[idx])\n",
    "        image = io.imread(path)\n",
    "        sample = {'image':image}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "class ToGray(object):\n",
    "    \"\"\"\n",
    "    Example:\n",
    "    tg = ToGray()\n",
    "    tg(dataset[0])\n",
    "    \"\"\"\n",
    "    \n",
    "    def __call__(self,sample):\n",
    "        image = sample['image']\n",
    "        \n",
    "        return {'image':1-rgb2gray(image)} # 1 - to invert\n",
    "    \n",
    "class Rescale(object):\n",
    "    \"\"\"\n",
    "    see https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image = sample['image']\n",
    "        \n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "\n",
    "        return {'image': img}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "young-judge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2c58022130>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = CustomDataset(\n",
    "    transform=transforms.Compose([ToGray(),Rescale((28,28))])\n",
    ")\n",
    "\n",
    "plt.imshow(dataset[0]['image'],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "impossible-disorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_loader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset, \n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "dataset_iter = iter(dataset_loader)\n",
    "batch = dataset_iter.next() \n",
    "\n",
    "image = batch['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "tribal-lebanon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(image.view(image.size(0),-1).float()).argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closing-childhood",
   "metadata": {},
   "source": [
    "We got 0, which is correct. It is not surprising as our accuracy is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "associate-partner",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ardimas/miniconda3/envs/pytorch_tutorial/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1446: UserWarning: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `test(ckpt_path='best')` to use and best model checkpoint and avoid this warning or `ckpt_path=trainer.checkpoint_callback.last_model_path` to use the last model.\n",
      "  rank_zero_warn(\n",
      "Restoring states from the checkpoint path at /home/ardimas/Documents/PROJECTS/pytorch_cv/lightning_logs/version_2/checkpoints/epoch=9-step=5500.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from checkpoint at /home/ardimas/Documents/PROJECTS/pytorch_cv/lightning_logs/version_2/checkpoints/epoch=9-step=5500.ckpt\n",
      "/home/ardimas/miniconda3/envs/pytorch_tutorial/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50f0a4065abd46f7ac189258011b41d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ardimas/miniconda3/envs/pytorch_tutorial/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                not been set for this class (_ResultMetric). The property determines if `update` by\n",
      "                default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                achieved and we recommend setting this to `False`.\n",
      "                We provide an checking function\n",
      "                `from torchmetrics.utilities import check_forward_no_full_state`\n",
      "                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                default for now) or if `full_state_update=False` can be used safely.\n",
      "                \n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.9642000198364258\n",
      "        test_loss           0.11869892477989197\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.11869892477989197, 'test_acc': 0.9642000198364258}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-marina",
   "metadata": {},
   "source": [
    "# Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-saturn",
   "metadata": {},
   "source": [
    "As I shared the other day, there is a new type of optimizer, i.e., https://github.com/IssamLaradji/sps\n",
    "\n",
    "So, let's compare it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "protective-minnesota",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1412\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type   | Params\n",
      "-----------------------------------------\n",
      "0 | input_layer   | Linear | 78.5 K\n",
      "1 | hidden_layer1 | Linear | 5.0 K \n",
      "2 | output_layer  | Linear | 510   \n",
      "-----------------------------------------\n",
      "84.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "84.1 K    Total params\n",
      "0.336     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ardimas/miniconda3/envs/pytorch_tutorial/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/ardimas/miniconda3/envs/pytorch_tutorial/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "176f6c924d2245edb9e3512492d02125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pl.seed_everything(1412) # --> DON'T FORGET this for consistency \n",
    "\n",
    "model = FNN(784, 100, 50, 10, optimizer='sps') # --> NEW\n",
    "\n",
    "try:\n",
    "    trainer = pl.Trainer(gpus=1,max_epochs=10)\n",
    "except Exception as e:\n",
    "    # most likely due to GPU, so fallback to non GPU\n",
    "    print(e)\n",
    "    trainer = pl.Trainer(max_epochs=10)\n",
    "    \n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-algorithm",
   "metadata": {},
   "source": [
    "The accuracy is now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "double-bangkok",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/ardimas/Documents/PROJECTS/pytorch_cv/lightning_logs/version_3/checkpoints/epoch=9-step=5500.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from checkpoint at /home/ardimas/Documents/PROJECTS/pytorch_cv/lightning_logs/version_3/checkpoints/epoch=9-step=5500.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d676916340429b974189caa2856a94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.9538999795913696\n",
      "        test_loss           0.15414927899837494\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.15414927899837494, 'test_acc': 0.9538999795913696}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-specific",
   "metadata": {},
   "source": [
    "Let's compare it using TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adolescent-escape",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 111424), started 0:10:44 ago. (Use '!kill 111424' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-78cdf8efd330e5b3\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-78cdf8efd330e5b3\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-marathon",
   "metadata": {},
   "source": [
    "The new optimizer is not better than Adam based on our simple experiment. In order to have a more general conclusion, you should investigate more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-sympathy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
