# Containerized API

## Introduction

Eventually you need to put your DL model into a use. 
Most likely, you'll deploy it as a backend application (at least for nowadays).
Speaking of nowadays, people commonly use containerization technologies to package an application.
Therefore, in this module, we will containerize our pretrained model and expose it as a RESTful API.
Specifically, we'll use Docker and FastAPI for the sake of simplicity.

A high-level view of our system is shown below.
An image to be classified will be passed to the backend service as a base64 string inside a JSON message in a POST request.
Note that other than a string you can use different types as specified in the [JSON spec](https://www.json.org/json-en.html), e.g., numbers.
However, it is just easier to use an existing encoder, e.g., base64 encoder.
The POST request will be directed to port 8000 on which the docker container listens.
The port 8000 is then mapped to port 80 on which the FastAPI application listens.
Next, the base64 string should be converted to a binary image file.
The image file can then be pre-processed such that rescaling or color conversion. 
Before being processed by our pretrained model, the data needs to be transformed to a tensor format.
Then, we can perform a feedforward calculation.
In addition, you can perform the GRAD-CAM to return a heatmapped image.
In this case, you can return it as a base64 string as a response for the earlier POST request.

```
                 ┌────────────────────────────────────────────────────────────────────┐
                 │                                                                    │
                 │          ┌───────────────────────────────────────────────────────┐ │
                 │:8000     │ :80                                                 │ │
                 │          │                                                       │ │
                 │          │               ┌─────────────┐       ┌─────────────┐   │ │
┌──────┐ POST    │ request  │  ┌──────┐     │             │       │     Your    │   │ │
│Client├─────────┼──────────┼─►│ HTTP ├─────►Preprocessing├───────►  Pretrained ├─┐ │ │
└──────┘ JSON    │          │  │Server│     │             │       │    Model    │ │ │ │
    ▲            │          │  └──────┘     └─────────────┘       └─────────────┘ │ │ │
    │ {img:base64}          │      base64 -> img         img->tensor              │ │ │
    │            │          │                                                     │ │ │
    │            │          │                                                     │ │ │
    │            │          │                   response                          │ │ │
    └────────────┼──────────┼─────────────────────────────────────────────────────┘ │ │
        JSON     │          │ FastAPI                             base64<-tensor    │ │
    {img:base64} │          └───────────────────────────────────────────────────────┘ │
                 │ Docker container -p 8000:80                                      │
                 └────────────────────────────────────────────────────────────────────┘
```

The following are a step-by-step procedure that we need to do.

1. Prepare a boilerplate for our pretrained model.
   
   In this step we are gonna build a containerized FastAPI application that can accept a POST request.
   
2. Specify our pretrained model dependency libraries
   
   We will specify what libraries required for our pretrained model. 
   In our case, we will describe them in a Python requirement text file and a dockerfile.

3. Inject our pytorch code inside the FastAPI app.
   
   We will insert our pytorch code inside a function that is called when the POST request is received.

Now, let's get over these steps one by one.

## 1. Preparing a Boilerplate for our Dockerized FastAPI Application 

You can follow the official tutorial from FastAPI [here](https://fastapi.tiangolo.com/deployment/docker/).
Or, you can follow what I usually do whenever I use FastAPI.
I'm using the following file structure.

```
.
├── Dockerfile
├── requirements.txt
└── api
    └── __init__.py
    └── main.py

```

Let's first use the following Dockerfile.

```dockerfile
FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7

COPY ./requirements.txt $pwd

COPY ./api /app/app
```

I'm gonna first use the following package as my first trial.
```
aiofiles==0.7.0
```

As for the main.py, try the following boilerplate.
```python

from fastapi import FastAPI
from fastapi.responses import RedirectResponse
from fastapi.middleware.cors import CORSMiddleware

from pydantic import BaseModel

from fastapi.openapi.docs import (
    get_swagger_ui_html,
)

# specify your API title
app = FastAPI(
        title="Your API",
        description="Your API: Documentation",
        version="0.1.0",
        docs_url=None,
        redoc_url=None,
        )

# CORS definition
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:8000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Swagger docs
@app.get("/docs", include_in_schema=False)
async def custom_swagger_ui_html():
    return get_swagger_ui_html(
        openapi_url=app.openapi_url,
        title=app.title,
        swagger_favicon_url="/static/logo.png",
    )

# Auto redirect to the doc page
@app.get("/", include_in_schema=False)
async def root():
    response = RedirectResponse(url='/docs')
    return response

# Request body specification
class ImageData(BaseModel):
    img: str

    class Config:
        schema_extra = {
                "example": {
                            "img": "{replace this with a base64 string}"
                }
        }

# Endpoint to perform an image recognition
@app.post("/image-recognition")
async def image_recognition(image_data: ImageData):

    return {
        "success": True,
        "gradcam": image_data.img
    }


```

Then, you can build an image named fastapi-boilerplate as:

```
sudo docker build -t fastapi-boilerplate .
```

And, run your container as:
```
sudo docker run -p 8000:80 fastapi-boilerplate
```

If everything runs smoothly, then you can open [http://localhost:8000](http://localhost:8000) and try your API via the Swagger interface that is automatically generated by FastAPI.

![./gifs/fastapi-boilerplate.gif](./gifs/fastapi-boilerplate.gif) 

## 2. Our Pretrained Model Dependencies

First, since we'll use the OpenCV library, it needs libgl1-mesa-glx library.
Therefore, you can add it in the Dockerfile as follows.


```dockerfile
FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7

RUN apt-get update && apt-get -y install libgl1-mesa-glx

COPY ./requirements.txt $pwd

RUN pip install -r requirements.txt

COPY ./api /app/app
```

Then, we can specify our python packages as:

```
torch==1.7.1  
torchvision==0.8.2  
pytorch-lightning==1.1.6 
aiofiles==0.7.0 
opencv-python==4.5.1.48
iteround==1.0.4
protobuf==3.20.1
```

You can add the following packages in the main.py, then build and run your Docker image and container in order to test if the dependencies are installed properly.

```python
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import pytorch_lightning as pl
from torchvision import transforms, models # --> new

from PIL import Image, ImageOps
import base64
from io import BytesIO
import cv2

from iteround import saferound
```

Then, open [http://localhost:8000](http://localhost:8000) and make sure that it can open on your browser.
If it does, then the dependencies are installed properly.

## 3. Insert Your PyTorch Code

For the impatient, you can actually refer to the final code under the directory [9-docker-fastapi-pytorch](./9-docker-fastapi-pytorch).
However, for easy troubleshooting, I first added the following to the end of the Dockerfile to enable the hot reload.

```dockerfile
CMD ["/start-reload.sh"]
```

Then, I also copied the checkpoint file (ckpt) to the directory where main.py exists.

Don't forget to build your image again.
Then, I run the container by using the following.

```sh
sudo docker run -p 8000:80 -v "$(pwd)/api/main.py:/app/app/main.py" fastapi-boilerplate
```

As for the main.py, the following code is the essential one to convert and transform the input base64 string.

```python
img = base64.b64decode(str(image_data.img)) 

img = Image.open(BytesIO(img)) 
img = ImageOps.exif_transpose(img)
img = img.convert("RGB")

transform = transforms.Compose(
        [
            transforms.Resize((256,256)),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ]
    )

img_tensor = transform(img).unsqueeze(0)
```

For the rest of the code, you can copy and paste it from [./8. Grad-CAM.ipynb](./8.%20Grad-CAM.ipynb).

Finally, you can test your API as follows.

![./gifs/final-dockerized-fastapi.gif](./gifs/final-dockerized-fastapi.gif)

